import torch
from torch.utils.data import Dataset, DataLoader, random_split
import pandas as pd
import numpy as np

class SchiebingerCountDataset(Dataset):
    """
    PyTorch Dataset for the preprocessed Schiebinger single-cell data.
    Loads data from a single .pt file containing counts and timepoints.
    """
    def __init__(self, data_file_path, device="cuda"):
        """
        Args:
            data_file_path (str): Path to the .pt file generated by the preprocessing script.
            device (str): The device to move tensors to ('cuda' or 'cpu').
        """
        print(f"Loading data from: {data_file_path}")
        
        # Load the entire data bundle from the single file
        data_bundle = torch.load(data_file_path, map_location=device)
        
        # Extract the required tensors
        self.counts = data_bundle['counts']
        self.timepoints = data_bundle['timepoints']
        
        # For reference, you can also access other metadata if needed
        self.cell_ids = data_bundle['cell_ids']
        self.gene_names = data_bundle['gene_names']
        
        print(f"Dataset created with {self.counts.shape[0]} cells and {self.counts.shape[1]} genes.")

    def __len__(self):
        """Returns the total number of cells in the dataset."""
        return self.counts.shape[0]

    def __getitem__(self, idx):
        """
        Retrieves a single data point (cell).

        Args:
            idx (int): The index of the data point.

        Returns:
            tuple: A tuple containing (cell_counts, timepoint).
                   This matches the `batch_data, pseudo_time` unpacking.
        """
        # Get the gene counts for the cell at the given index
        x = self.counts[idx]
        
        # Get the corresponding timepoint (label)
        y = self.timepoints[idx]
        
        return x, y

def create_count_dataloaders(
    data_file,
    batch_size=16,
    validation_split=0.0,
    shuffle=True,
    device="cuda"
):
    """
    Creates PyTorch DataLoader(s) for the Schiebinger dataset.

    This function maintains the same API as the original dataloader but works
    with the new single-file .pt format.

    Args:
        data_file (str): Path to the single .pt file containing all data.
        batch_size (int): The number of samples per batch.
        validation_split (float): The fraction of data to use for validation (0.0 to 1.0).
        shuffle (bool): Whether to shuffle the training data each epoch.
        device (str): The device to load data onto ('cuda' or 'cpu').

    Returns:
        If validation_split > 0:
            (DataLoader, DataLoader): A tuple of (train_loader, val_loader).
        Else:
            DataLoader: A single training loader.
    """
    # 1. Instantiate the new dataset
    dataset = SchiebingerCountDataset(data_file, device=device)

    # 2. Handle the optional validation split
    if validation_split > 0.0:
        if not (0 < validation_split < 1):
            raise ValueError("validation_split must be between 0 and 1.")
        
        # Calculate the sizes of the splits
        val_size = int(len(dataset) * validation_split)
        train_size = len(dataset) - val_size
        
        print(f"Splitting data: {train_size} training samples and {val_size} validation samples.")
        
        # Perform the random split
        train_ds, val_ds = random_split(dataset, [train_size, val_size])
        
        # Create DataLoaders for both training and validation sets
        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)
        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False) # No need to shuffle validation data
        
        return train_loader, val_loader

    # 3. If no split, return a single DataLoader for the entire dataset
    print("Creating a single training loader with no validation split.")
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

# Example Usage:
if __name__ == '__main__':
    # This is a placeholder for a real file path.
    # Replace this with the actual path to your generated .pt file.
    DUMMY_DATA_FILE = './HVGs/schiebinger_hvg_tensor_trunk-both_2000hvg.pt'

    # Check if a dummy file exists, otherwise create one for demonstration
    if not os.path.exists(DUMMY_DATA_FILE):
        print("Creating a dummy data file for demonstration purposes...")
        os.makedirs('./HVGs', exist_ok=True)
        dummy_data = {
            'counts': torch.randn(1000, 2000), # 1000 cells, 2000 genes
            'timepoints': torch.rand(1000, 1) * 18, # 1000 timepoints
            'cell_ids': [f'cell_{i}' for i in range(1000)],
            'gene_names': [f'gene_{i}' for i in range(2000)],
            'metadata_df': pd.DataFrame({'day': torch.rand(1000).numpy() * 18})
        }
        torch.save(dummy_data, DUMMY_DATA_FILE)


    # --- Test Case 1: Create a single training loader ---
    print("\n--- TEST CASE 1: No validation split ---")
    # Determine device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    train_loader = create_count_dataloaders(
        data_file=DUMMY_DATA_FILE,
        batch_size=32,
        validation_split=0.0,
        shuffle=True,
        device=device
    )

    # Check the output
    for i, batch in enumerate(train_loader):
        batch_data, pseudo_time = batch
        print(f"Batch {i+1}:")
        print(f"  Data shape: {batch_data.shape}")      # Expected: [32, 2000]
        print(f"  Time shape: {pseudo_time.shape}")      # Expected: [32, 1]
        print(f"  Data device: {batch_data.device}")
        if i == 1: # Stop after a couple of batches
            break
            
    # --- Test Case 2: Create training and validation loaders ---
    print("\n--- TEST CASE 2: With validation split ---")
    train_loader_split, val_loader_split = create_count_dataloaders(
        data_file=DUMMY_DATA_FILE,
        batch_size=32,
        validation_split=0.2, # 20% for validation
        shuffle=True,
        device=device
    )
    
    # Check the training loader from the split
    print("\nChecking the split training loader...")
    for i, batch in enumerate(train_loader_split):
        batch_data, pseudo_time = batch
        print(f"Train Batch {i+1}: Data shape: {batch_data.shape}, Time shape: {pseudo_time.shape}")
        if i == 1:
            break

    # Check the validation loader from the split
    print("\nChecking the split validation loader...")
    for i, batch in enumerate(val_loader_split):
        batch_data, pseudo_time = batch
        print(f"Validation Batch {i+1}: Data shape: {batch_data.shape}, Time shape: {pseudo_time.shape}")
        if i == 1:
            break
