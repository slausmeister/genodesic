{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e867a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e0e7b",
   "metadata": {},
   "source": [
    "Notebook Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b9e42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trunk = \"2i\"  # Options: \"serum\", \"2i\", \"both\"\n",
    "\n",
    "ReextractHVGs = False\n",
    "RetrainAutoencoder = False\n",
    "RetrainDensityModel = False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf67d81",
   "metadata": {},
   "source": [
    "# 0: Downloading the Schiebinger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3070d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Schiebinger Dataset Downloader ---\n",
      "Directory Data/Schiebinger already exists and is not empty.\n",
      "Continuing: will attempt to extract/unpack any archives found in this directory.\n",
      "Skipping cleanup as no new download was performed.\n",
      "---\n",
      "Dataset is located in: Data/Schiebinger\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./SchiebingerDownload.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3fba69",
   "metadata": {},
   "source": [
    "# 1: Extracting HVGs of chosen Branch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48547b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"Data/Schiebinger\"\n",
    "output_directory = \"Pipeline/HVGs\"\n",
    "num_highly_variable_genes = 2000\n",
    "\n",
    "# Define the specific output filename\n",
    "tensor_filename = f\"schiebinger_hvg_tensor_trunk-{trunk}_{num_highly_variable_genes}hvg.pt\"\n",
    "tensor_path = os.path.join(output_directory, tensor_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26c36f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File schiebinger_hvg_tensor_trunk-2i_2000hvg.pt already exists. Skipping HVG extraction.\n"
     ]
    }
   ],
   "source": [
    "from Pipeline.firstSelectHVGs import run_hvg_extraction\n",
    "\n",
    "if os.path.exists(tensor_path) and not ReextractHVGs:\n",
    "    print(f\"File {tensor_filename} already exists. Skipping HVG extraction.\")\n",
    "else:\n",
    "    print(f\"Starting HVG extraction...\")\n",
    "    # --- 3. Call the function directly with your parameters ---\n",
    "    # This is clean, robust, and doesn't involve any argparse messiness.\n",
    "    hvg_fig = run_hvg_extraction(\n",
    "        data_dir=data_directory,\n",
    "        output_dir=output_directory,\n",
    "        output_file=tensor_filename,\n",
    "        trunk=trunk,\n",
    "        n_hvg=num_highly_variable_genes,\n",
    "        min_counts=2000,  \n",
    "        max_counts=50000,\n",
    "        min_cells=50,\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    if hvg_fig:\n",
    "        print(\"\\nDisplaying diagnostic plot:\")\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9291c013",
   "metadata": {},
   "source": [
    "# 2: Training the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91baf53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = f\"Models/Autoencoder/trunk-{trunk}.pt\"\n",
    "latent_save_path = f\"LatentSpace/trunk-{trunk}_latent.pt\"\n",
    "\n",
    "bottleneck = 24\n",
    "\n",
    "latent_dims = [660, 220 , 66, bottleneck]\n",
    "\n",
    "batch_size = 64\n",
    "overdispersion = 0.3\n",
    "\n",
    "num_epochs = 30\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9a821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at Models/Autoencoder/trunk-2i.pt. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "from Pipeline.secondTrainAutoencoder import run_autoencoder_training\n",
    "\n",
    "if os.path.exists(model_save_path) and not RetrainAutoencoder:\n",
    "    print(f\"Model already exists at {model_save_path}. Skipping training.\")\n",
    "else:\n",
    "    # Call the training function directly with clear, explicit parameters.\n",
    "    # This is robust, readable, and provides full IDE support.\n",
    "    run_autoencoder_training(\n",
    "        tensor_file=tensor_path,\n",
    "        model_save_path=model_save_path,\n",
    "        latent_save_path=latent_save_path,\n",
    "        latent_dims=latent_dims,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        overdispersion=overdispersion,\n",
    "        lr=5e-4,      \n",
    "        val_split=0.2, \n",
    "        debug=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3770bc66",
   "metadata": {},
   "source": [
    "## 2.1: Visualizing the Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22df9c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuML found. Using GPU for UMAP acceleration.\n",
      "Loaded 172756 latent vectors.\n"
     ]
    }
   ],
   "source": [
    "from Genodesic.Visualizers import UMAP3D\n",
    "\n",
    "latent_data_bundle = torch.load(latent_save_path)\n",
    "\n",
    "# Extract the numpy arrays for plotting\n",
    "latent_reps = latent_data_bundle['latent_reps'].numpy()\n",
    "timepoints = latent_data_bundle['timepoints'].numpy().flatten()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Loaded {latent_reps.shape[0]} latent vectors.\")\n",
    "\n",
    "# UMAP3D(\n",
    "#     latent_reps=latent_reps,\n",
    "#     color_by_timepoints=timepoints,\n",
    "#     title=f\"Latent Space UMAP (Trunk: {trunk})\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4b9b5",
   "metadata": {},
   "source": [
    "# 3: Setting up Density Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa8d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts.train import run_training\n",
    "\n",
    "model_save_path = \"Models/DensityModels/vpsde.pt\"\n",
    "\n",
    "notebook_overrides = {\n",
    "    \"model_type\": \"vpsde\", # Options: \"vpsde\", \"otcfm\", \"rqnsf\"\n",
    "    \"data_file\": latent_save_path, \n",
    "    \"model_save_path\": model_save_path,\n",
    "    \"dim\": bottleneck,\n",
    "    \"num_epochs\": 50,\n",
    "    \"batch_size\": 64\n",
    "}\n",
    "\n",
    "if RetrainDensityModel:\n",
    "    trained_model = run_training(config_overrides=notebook_overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8079214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Genodesic.DensityModels import OptimalFlowModel, ScoreSDEModel, RQNSFModel\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "model = ScoreSDEModel.from_checkpoint(model_save_path, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f9660d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load the saved paths\n",
    "saved_paths = torch.load('Genodesic_tutorial_paths.pt')\n",
    "phi_initial = saved_paths['phi_initial']\n",
    "phi_relaxed = saved_paths['phi_relaxed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "816085d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating segment lengths...\n",
      "Calculating midpoint log-densities...\n",
      "Calculating Fermat lengths...\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Assume these functions are defined in your Genodesic library\n",
    "from Genodesic.PathTools.PathEvaluation import (\n",
    "    calculate_euclidean_segment_lengths,\n",
    "    calculate_path_density,\n",
    "    calculate_fermat_length,\n",
    ")\n",
    "from Genodesic.PathTools import DensityBasedResampling # And other utilities\n",
    "\n",
    "# ===================================================================\n",
    "#\n",
    "# 1. DATA PROCESSING FUNCTION\n",
    "#\n",
    "# ===================================================================\n",
    "\n",
    "def analyze_paths(\n",
    "    paths_to_analyze: Dict[str, torch.Tensor],\n",
    "    model,\n",
    "    beta: float,\n",
    "    density_s_steps: int,\n",
    "    fermat_s_steps: int\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Analyzes a dictionary of paths, computing all required metrics.\n",
    "\n",
    "    Args:\n",
    "        paths_to_analyze: A dictionary where keys are path names (e.g., 'Initial')\n",
    "                          and values are the path tensors.\n",
    "        model: The density/score model.\n",
    "        beta: The beta parameter for the Fermat metric.\n",
    "        density_s_steps: Interpolation steps for density calculation.\n",
    "        fermat_s_steps: Integration steps for Fermat length.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the calculated metrics for each path.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Path Analysis ---\")\n",
    "    results = {}\n",
    "    for name, path_tensor in paths_to_analyze.items():\n",
    "        print(f\"Processing '{name}' path...\")\n",
    "        \n",
    "        # Calculate all metrics for the current path\n",
    "        lengths = calculate_euclidean_segment_lengths(path_tensor)\n",
    "        densities = calculate_path_density(path_tensor, model, density_s_steps)\n",
    "        fermat_length = calculate_fermat_length(path_tensor, model, beta, fermat_s_steps)\n",
    "        \n",
    "        # Store results in a structured dictionary\n",
    "        results[name] = {\n",
    "            \"path_tensor\": path_tensor,\n",
    "            \"lengths\": lengths,\n",
    "            \"total_length\": np.sum(lengths),\n",
    "            \"densities\": densities,\n",
    "            \"mean_log_density\": np.mean(densities),\n",
    "            \"fermat_length\": fermat_length,\n",
    "        }\n",
    "    print(\"--- Analysis Complete ---\\n\")\n",
    "    return results\n",
    "\n",
    "# ===================================================================\n",
    "#\n",
    "# 2. MODULAR PLOTTING & REPORTING FUNCTIONS\n",
    "#\n",
    "# ===================================================================\n",
    "\n",
    "def plot_density_distributions(\n",
    "    analysis_results: Dict[str, Dict[str, Any]],\n",
    "    colors: Dict[str, str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the log-density distribution for an arbitrary number of paths.\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: The output from the analyze_paths function.\n",
    "        colors: An optional dictionary mapping path names to plot colors.\n",
    "    \"\"\"\n",
    "    print(\"Plotting log-density distributions...\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    default_colors = ['#FF6347', '#3CB371', '#1E90FF', '#FFD700', '#DB7093']\n",
    "    \n",
    "    for i, (name, data) in enumerate(analysis_results.items()):\n",
    "        color = colors.get(name) if colors else default_colors[i % len(default_colors)]\n",
    "        \n",
    "        plt.hist(\n",
    "            data['densities'],\n",
    "            bins=40,\n",
    "            alpha=0.7,\n",
    "            label=f\"{name} Path (Mean Log-Density: {data['mean_log_density']:.2f})\",\n",
    "            density=True,\n",
    "            color=color\n",
    "        )\n",
    "        \n",
    "    plt.title('Distribution of Midpoint Log-Densities', fontsize=16)\n",
    "    plt.xlabel('Log-Density', fontsize=12)\n",
    "    plt.ylabel('Normalized Frequency', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "def print_path_statistics(analysis_results: Dict[str, Dict[str, Any]], beta: float):\n",
    "    \"\"\"Prints a formatted summary table of the analysis results.\"\"\"\n",
    "    print(\"--- Path Statistics ---\")\n",
    "    for name, data in analysis_results.items():\n",
    "        print(f\"\\n>> {name} Path:\")\n",
    "        print(f\"  Euclidean Length: Total={data['total_length']:.3f}, Mean Seg={np.mean(data['lengths']):.3f}\")\n",
    "        print(f\"  Log-Density:      Mean={data['mean_log_density']:.3f}, Std={np.std(data['densities']):.3f}\")\n",
    "        print(f\"  Fermat Length:    {data['fermat_length']:.3f} (beta={beta})\")\n",
    "\n",
    "# ===================================================================\n",
    "#\n",
    "# 3. MAIN SCRIPT\n",
    "#\n",
    "# ===================================================================\n",
    "\n",
    "# --- Configuration ---\n",
    "BETA_PARAM = 0.33\n",
    "DENSITY_STEPS = 3\n",
    "INTEGRATION_STEPS = 4\n",
    "\n",
    "# --- Load and Define Paths ---\n",
    "# This is now the main place to add or remove paths for analysis.\n",
    "saved_paths = torch.load('Genodesic_tutorial_paths.pt')\n",
    "\n",
    "# Add any other paths you want to compare\n",
    "# resampled_path = DensityBasedResampling(...)\n",
    "# test_path = DensityBasedResampling(...)\n",
    "\n",
    "paths = {\n",
    "    \"Initial\": saved_paths['phi_initial'],\n",
    "    \"Relaxed\": saved_paths['phi_relaxed'],\n",
    "    \"Negative\": negative_beta, \n",
    "    # \"Test\": test_path           # Example: Easy to add new paths\n",
    "}\n",
    "\n",
    "# Define corresponding colors for plotting (optional but recommended)\n",
    "path_colors = {\n",
    "    \"Initial\": \"#FF6347\",  # Tomato Red\n",
    "    \"Relaxed\": \"#3CB371\",  # Medium Sea Green\n",
    "    \"Resampled\": \"#1E90FF\", # Dodger Blue\n",
    "    \"Test\": \"#FFD700\"     # Gold\n",
    "}\n",
    "\n",
    "\n",
    "# --- Run Analysis and Reporting ---\n",
    "# 1. Process all paths in one go\n",
    "path_analysis_results = analyze_paths(\n",
    "    paths, model, BETA_PARAM, DENSITY_STEPS, INTEGRATION_STEPS\n",
    ")\n",
    "\n",
    "# 2. Print the summary statistics\n",
    "print_path_statistics(path_analysis_results, BETA_PARAM)\n",
    "\n",
    "# 3. Plot the density distributions\n",
    "plot_density_distributions(path_analysis_results, colors=path_colors)\n",
    "\n",
    "# Note: Your pseudotime plotting functions are already well-refactored\n",
    "# to handle multiple paths, so they can be used as-is with the new structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My SIF Kernel",
   "language": "python",
   "name": "sif_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
